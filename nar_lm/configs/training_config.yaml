# configs/training_config_improved.yaml
# Enhanced training configuration with larger dataset

# Dataset 
dataset_name: "wikitext"
dataset_config: "wikitext-103-v1"
num_train_samples: 50000  # Increased from 10000
num_val_samples: 5000     # Increased from 1000
num_test_samples: 2000    # Increased from 1000
max_char_length: 1066     # Increased from 768
val_split: 0.1

# Training parameters
batch_size: 8
learning_rate: 3e-5       # Slightly reduced from 5e-5
weight_decay: 0.01
max_epochs: 20            # Increased from 15
steps_per_epoch: 6250     # num_train_samples / batch_size
use_lr_scheduler: true
gradient_clip_val: 0.5

# Hardware/Performance
num_workers: 4
precision: "16-mixed"
accumulate_grad_batches: 4  # Increased from 2 - effective batch size of 32

# Logging
log_every_n_steps: 100
save_top_k: 3

# Paths
output_dir: "./checkpoints"
log_dir: "./logs"